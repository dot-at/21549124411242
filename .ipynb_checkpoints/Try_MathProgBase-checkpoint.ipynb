{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok, let's try it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MathProgBase\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna need a solver. Let's try IPOPT (Mosek doesn't work on Juliabox anyway, since it requires a license):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Ipopt\n",
    "mysolver = IpoptSolver()\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MathProgBase exports almost nothing, so the following is qualified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MathProgBase.NonlinearModel(mysolver)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some random things involving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typeof(model) = Ipopt.IpoptMathProgModel\n",
      "supertype(typeof(model)) = MathProgBase.SolverInterface.AbstractNonlinearModel\n"
     ]
    }
   ],
   "source": [
    "@show typeof(model)\n",
    "@show supertype(typeof(model))\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I understand correctly, I now have to create a subtype of `MathProgBase.AbstractNLPEvaluator`. I suppose, it could hold some data for the instance. \n",
    "Then, I have to implement methods (initialization, evaluating gradients, Hessians, etc) for it.  Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type My_Eval <: MathProgBase.AbstractNLPEvaluator\n",
    "    # I'm going to:\n",
    "    # minimize the natural entropy,\n",
    "    # subject to being a probability distribution\n",
    "    # with this range:\n",
    "    n::Int\n",
    "    My_Eval(_n) = new(Int(_n))\n",
    "end\n",
    "numo_vars(e::My_Eval) :: Int = e.n\n",
    "numo_constraints(e::My_Eval) :: Int = 1\n",
    "constraints_lowerbounds_vec(e::My_Eval) :: Vector{Float64} = [1.]\n",
    "constraints_upperbounds_vec(e::My_Eval) :: Vector{Float64} = [1.]\n",
    "vars_lowerbounds_vec(e::My_Eval) :: Vector{Float64} = [0.   for j=1:e.n]\n",
    "vars_upperbounds_vec(e::My_Eval) :: Vector{Float64} = [Inf  for j=1:e.n]\n",
    "features_list(e::My_Eval) :: Vector{Symbol}  = [:Grad, :Jac, :JacVec, :Hess, :HessVec]\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Methods for the NLP Evaluator\n",
    "Documentation of the functions is here: [http://mathprogbasejl.readthedocs.io/en/latest/nlp.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import MathProgBase.initialize\n",
    "import MathProgBase.features_available\n",
    "import MathProgBase.eval_f\n",
    "import MathProgBase.eval_g\n",
    "import MathProgBase.eval_grad_f\n",
    "import MathProgBase.jac_structure\n",
    "import MathProgBase.hesslag_structure\n",
    "import MathProgBase.eval_jac_g\n",
    "import MathProgBase.eval_jac_prod\n",
    "import MathProgBase.eval_jac_prod_t\n",
    "import MathProgBase.eval_hesslag_prod\n",
    "import MathProgBase.eval_hesslag\n",
    "import MathProgBase.isobjlinear\n",
    "import MathProgBase.isobjquadratic\n",
    "import MathProgBase.isconstrlinear\n",
    "import MathProgBase.obj_expr\n",
    "import MathProgBase.constr_expr\n",
    "import MathProgBase.getreducedcosts\n",
    "import MathProgBase.getconstrduals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to give a name to the type of my model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typealias My_Model typeof(model)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize()\n",
    "Let's define the first function:\n",
    "`initialize(d::AbstractNLPEvaluator, requested_features::Vector{Symbol})`\n",
    "The following is from the MathProgBase documentation\n",
    "\n",
    "Must be called before any other methods.\n",
    "\n",
    "The vector `requested_features` lists features requested by the solver.  These may include `:Grad` for gradients of the objective function, `:Jac` for explicit Jacobians of the constraing functions, `:JacVec` for Jacobian-vector products, `:HessVec` for Hessian-vector and Hessian-of-Lagrangian-vector products, `:Hess` for explicit Hessians and Hessian-of-Lagrangians, and `:ExprGraph` for expression graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function initialize(eval::My_Eval, requested_features::Vector{Symbol})\n",
    "    for feat in requested_features\n",
    "        if feat ∉ features_list(eval)\n",
    "            error(\"initialize(My_Eval): I don't have the feature $feat, sorry!\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features_available()\n",
    "`features_available(d::AbstractNLPEvaluator)` returns the subset of features available for this problem instance, as a list of symbols in the same format as in initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_available(eval::My_Eval) = features_list(eval)\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Objective and Constraint Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_f()\n",
    "`eval_f(d::AbstractNLPEvaluator, x)` evaluates f(x), returning a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entropy{T<:AbstractFloat}(t::T)::T = ( t > 0 ?   -t*log(t)  :  T(0) )\n",
    "eval_f{T<:AbstractFloat}(eval::My_Eval, x::Vector{T}) :: T  = entropy.(x) |> sum\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_g()\n",
    "`eval_g(d::AbstractNLPEvaluator, g, x)`: evaluate *g(x)*, storing the result in the vector *g* which must be of the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function eval_g{T<:AbstractFloat}(eval::My_Eval, g::Vector{T}, x::Vector{T})  :: T\n",
    "    g[1] = sum(x)\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_grad_f()\n",
    "`eval_grad_f(d::AbstractNLPEvaluator, g, x)` evaluates *∇f(x)* as a dense vector, storing the result in the vector *g* which must be of the appropriate size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function eval_grad_f{T<:AbstractFloat}(eval::My_Eval, g::Vector{T}, x::Vector{T}) :: Void\n",
    "    g .= - log.(x) .- T(1)\n",
    "    return\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian of the Constraint Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jac_structure()\n",
    "`jac_structure(d::AbstractNLPEvaluator)` returns the sparsity structure of the Jacobian matrix of the constraint map *g*. The sparsity structure is assumed to be independent of the point *x* at which the map is evaluated.\n",
    "\n",
    "The function returns a tuple *(I,J)* where *I* contains the row indices and *J* contains the column indices of each structurally nonzero element.  These indices are not required to be sorted and can contain duplicates, in which case the solver should combine the corresponding elements by adding them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jac_structure(eval::My_Eval) = ( ones(eval.n) , collect(1:eval.n) ) \n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_jac_g()\n",
    "`eval_jac_g(d::AbstractNLPEvaluator, J, x)` evaluates the sparse Jacobian matrix.  The result is stored in the vector *J* in the same order as the indices returned by `jac_structure()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function eval_jac_g{T<:AbstractFloat}(eval::My_Eval, J::Vector{T}, x::Vector{T}) :: Void\n",
    "    J .= T(1)\n",
    "    return\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_jac_prod()\n",
    "`eval_jac_prod(d::AbstractNLPEvaluator, y, x, w)` computes the product of the Jacobian of the constraint map with the vector *w*, storing the result in the vector *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function eval_jac_prod{T<:AbstractFloat}(eval::My_Eval, y::Vector{T}, x::Vector{T}, w::Vector{T}) :: Void\n",
    "    y .= w\n",
    "    return\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_jac_prod_t()\n",
    "`eval_jac_prod_t(d::AbstractNLPEvaluator, y, x, w)` computes the product of the transpose of the Jacobian of the constraint map with the vector *w*, storing the result in the vector *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function eval_jac_prod_t{T<:AbstractFloat}(eval::My_Eval, y::Vector{T}, x::Vector{T}, w::Vector{T}) :: Void\n",
    "    y .= w[1]\n",
    "    return\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian of the Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hesslag_structure()\n",
    "`hesslag_structure(d::AbstractNLPEvaluator)` returns the sparsity structure of the Hessian with respect to *x* of the Lagrangian. The Lagrangian is\n",
    "* *L(x, (σ,μ) ) = σ f(x) + μ' g(x)*\n",
    "\n",
    "The structure must be returned as a tuple *(I,J)* where *I* contains the row indices and *J* contains the column indices of each structurally nonzero element. These indices are not required to be sorted and can contain duplicates, in which case the solver should combine the corresponding elements by adding them together. Any mix of lower and upper-triangular indices is valid. Elements *(i,j)* and *(j,i)*, if both present, are treated as duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hesslag_structure(eval::My_Eval) = ( collect(1:eval.n), collect(1:eval.n) )\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_hesslag()\n",
    "`eval_hesslag(d::AbstractNLPEvaluator, H, x, σ, μ)` takes a scalar weight σ and vector of constraint weights μ, and computes the Hessian wrt *x* of the Lagrangian as a sparse matrix.  The result is stored in the vector *H* in the same order as the indices returned by `hesslag_structure()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function eval_hesslag{T<:AbstractFloat}(eval::My_Eval, H::Vector{T}, x::Vector{T}, σ::T, μ::Vector{T}) :: Void\n",
    "    H .=  - σ ./ x\n",
    "    println(\"Marco\")\n",
    "    return\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval_hesslag_prod()\n",
    "The  function `eval_hesslag_prod(d::AbstractNLPEvaluator, h, x, v, σ, μ)` takes a scalar weight σ and vector of constraint weights μ, and computes the product of \n",
    "* the Hessian of the Lagrangian wrt *x* at *(x,(σ, μ))*\n",
    "* with the vector *v*.\n",
    "\n",
    "The result is stored in the vector *h*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function eval_hesslag_prod{T<:AbstractFloat}(eval::My_Eval, h::Vector{T}, x::Vector{T}, v::Vector{T}, σ::T, μ::Vector{T}) :: Void\n",
    "    h .= -v .* σ ./ x[j]\n",
    "    println(\"Polo\")\n",
    "    return\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isobjlinear()\n",
    "`isobjlinear(d::AbstractNLPEvaluator)` returns true, if the objective function is known to be linear, false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isobjlinear(eval::My_Eval) = false\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isobjquadratic()\n",
    "`isobjquadratic(d::AbstractNLPEvaluator)` returns true, if the objective function is known to be quadratic (convex or nonconvex), false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isobjquadratic(eval::My_Eval) = false\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isconstrlinear()\n",
    "`isconstrlinear(d::AbstractNLPEvaluator, i)` returns true, if the *i*th constraint is known to be linear, false otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isconstrlinear(eval::My_Eval, i) = true\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More\n",
    "This looks like something I don't need:\n",
    "* `obj_expr(d::AbstractNLPEvaluator)`\n",
    "* `constr_expr(d::AbstractNLPEvaluator, i)`\n",
    "\n",
    "I have no idea what this is supposed to be:\n",
    "* `getreducedcosts(m::AbstractNonlinearModel)`\n",
    "* `getconstrduals(m::AbstractNonlinearModel)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembly\n",
    "Let's put it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Base.Test\n",
    "\n",
    "function mytest(n::Int, solver=MathProgBase.defaultNLPsolver)\n",
    "    \n",
    "    const model = MathProgBase.NonlinearModel(solver)\n",
    "    const myeval = My_Eval(n)\n",
    "    const lb = constraints_lowerbounds_vec(myeval)\n",
    "    const ub = constraints_upperbounds_vec(myeval)\n",
    "    const l = vars_lowerbounds_vec(myeval)\n",
    "    const u = vars_upperbounds_vec(myeval)\n",
    "\n",
    "    MathProgBase.loadproblem!(model, numo_vars(myeval), numo_constraints(myeval), l, u, lb, ub, :Max, myeval)\n",
    "\n",
    "    MathProgBase.optimize!(model)\n",
    "    stat = MathProgBase.status(model)\n",
    "\n",
    "    @test stat == :Optimal\n",
    "    @show MathProgBase.getobjval(model)\n",
    "    objvaldist = abs( log(n) - MathProgBase.getobjval(model) )*10^9\n",
    "    println(\"Distance from true optimum: $objvaldist\")\n",
    "    \n",
    "    x = MathProgBase.getsolution(model)\n",
    "    for j=1:n\n",
    "        @test_approx_eq_eps x[j] 1./n 1.e-10\n",
    "    end\n",
    "\n",
    "    @test_approx_eq_eps MathProgBase.getobjval(model) log(n)  1.e-300\n",
    "\n",
    "#    # Test that a second call to optimize! works\n",
    "#    MathProgBase.setwarmstart!(m,[1,5,5,1])\n",
    "#    MathProgBase.optimize!(m)\n",
    "#    stat = MathProgBase.status(m)\n",
    "#    @test stat == :Optimal\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marco\n",
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit http://projects.coin-or.org/Ipopt\n",
      "******************************************************************************\n",
      "\n",
      "This is Ipopt version 3.12.1, running with linear solver mumps.\n",
      "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
      "\n",
      "Number of nonzeros in equality constraint Jacobian...:      100\n",
      "Number of nonzeros in inequality constraint Jacobian.:        0\n",
      "Number of nonzeros in Lagrangian Hessian.............:      100\n",
      "\n",
      "Total number of variables............................:      100\n",
      "                     variables with only lower bounds:      100\n",
      "                variables with lower and upper bounds:        0\n",
      "                     variables with only upper bounds:        0\n",
      "Total number of equality constraints.................:        1\n",
      "Total number of inequality constraints...............:        0\n",
      "        inequality constraints with only lower bounds:        0\n",
      "   inequality constraints with lower and upper bounds:        0\n",
      "        inequality constraints with only upper bounds:        0\n",
      "\n",
      "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
      "   0 -4.6051666e+00 1.00e-06 1.11e-15  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
      "Marco\n",
      "   1 -4.6051702e+00 1.11e-15 1.50e-09  -3.8 1.00e-08    -  1.00e+00 1.00e+00h  1\n",
      "Marco\n",
      "   2 -4.6051702e+00 6.66e-16 1.84e-11  -5.7 1.16e-17    -  1.00e+00 1.00e+00   0\n",
      "Marco\n",
      "   3 -4.6051702e+00 2.22e-16 2.51e-14  -8.6 6.89e-18    -  1.00e+00 1.00e+00   0\n",
      "   4 -4.6051702e+00 2.22e-16 1.25e-18 -12.9 2.49e-18    -  1.00e+00 1.00e+00T  0\n",
      "\n",
      "Number of Iterations....: 4\n",
      "\n",
      "                                   (scaled)                 (unscaled)\n",
      "Objective...............:  -4.6051701859880917e-08   -4.6051701859880918e+00\n",
      "Dual infeasibility......:   1.2543830029897141e-18    1.2543830029897141e-10\n",
      "Constraint violation....:   2.2204460492503131e-16    2.2204460492503131e-16\n",
      "Complementarity.........:   1.2544302826374089e-13    1.2544302826374090e-05\n",
      "Overall NLP error.......:   1.2544302826374089e-13    1.2544302826374090e-05\n",
      "\n",
      "\n",
      "Number of objective function evaluations             = 5\n",
      "Number of objective gradient evaluations             = 5\n",
      "Number of equality constraint evaluations            = 5\n",
      "Number of inequality constraint evaluations          = 0\n",
      "Number of equality constraint Jacobian evaluations   = 5\n",
      "Number of inequality constraint Jacobian evaluations = 0\n",
      "Number of Lagrangian Hessian evaluations             = 4\n",
      "Total CPU secs in IPOPT (w/o function evaluations)   =      0.208\n",
      "Total CPU secs in NLP function evaluations           =      0.084\n",
      "\n",
      "EXIT: Optimal Solution Found.\n",
      "MathProgBase.getobjval(model) = 4.605170185988092\n",
      "Distance from true optimum: 0.0\n"
     ]
    }
   ],
   "source": [
    "mytest(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
